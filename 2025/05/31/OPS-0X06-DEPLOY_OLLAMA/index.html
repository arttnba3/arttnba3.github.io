

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/avatar.png">
  <link rel="icon" href="/img/avatar.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="arttnba3">
  <meta name="keywords" content="">
  
    <meta name="description" content="羊驼：会者不难">
<meta property="og:type" content="article">
<meta property="og:title" content="【OPS.0x06】使用 Ollama 本地部署轻量LLM">
<meta property="og:url" content="https://arttnba3.github.io/2025/05/31/OPS-0X06-DEPLOY_OLLAMA/index.html">
<meta property="og:site_name" content="arttnba3&#39;s blog">
<meta property="og:description" content="羊驼：会者不难">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2025/03/12/4NYwdKkcIWHLVAU.png">
<meta property="article:published_time" content="2025-05-30T18:17:33.000Z">
<meta property="article:modified_time" content="2025-07-03T14:47:11.733Z">
<meta property="article:author" content="arttnba3">
<meta property="article:tag" content="Linux">
<meta property="article:tag" content="运维">
<meta property="article:tag" content="DeepSeek">
<meta property="article:tag" content="Ollama">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://s2.loli.net/2025/03/12/4NYwdKkcIWHLVAU.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>【OPS.0x06】使用 Ollama 本地部署轻量LLM - arttnba3&#39;s blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"arttnba3.github.io","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":"§"},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"><link rel="alternate" href="/atom.xml" title="arttnba3's blog" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 80vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>arttnba3&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" target="_blank" rel="noopener" href="https://github.com/arttnba3">
                <i class="iconfont icon-github-fill"></i>
                GitHub
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/atom.xml">
                <i class="iconfont icon-rss"></i>
                rss
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://s2.loli.net/2025/03/12/zHIKOY2BFACjDWy.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="【OPS.0x06】使用 Ollama 本地部署轻量LLM"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-05-31 04:17" pubdate>
          2025年5月31日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          9.7k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          81 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
        <div class="scroll-down-bar">
          <i class="iconfont icon-arrowdown"></i>
        </div>
      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">【OPS.0x06】使用 Ollama 本地部署轻量LLM</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：2025年7月4日 凌晨
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <p>羊驼：会者不难</p>
<span id="more"></span>

<h1 id="0x00-一切开始之前"><a href="#0x00-一切开始之前" class="headerlink" title="0x00. 一切开始之前"></a>0x00. 一切开始之前</h1><p>众所周知最近这几年大语言模型挺火的，包括今年年初 DeepSeek-R1 的爆火也让同是华人的笔者感到自豪（毕竟 DeepSeek 团队是“全华班”），当然更多的闲话在这里继续瞎扯没啥意义，总而言之笔者也想在本地自己部署一些轻量级的 LLM 玩一玩，包括 DeepSeek、QWEN 之类的，看了一圈发现 Ollama 方案还是挺方便的，因此简单写一篇博客讲讲怎么使用 Ollama 自行部署 LLM</p>
<h1 id="0x01-安装并使用-Ollama"><a href="#0x01-安装并使用-Ollama" class="headerlink" title="0x01. 安装并使用 Ollama"></a>0x01. 安装并使用 Ollama</h1><p>Ollama 是一个能够非常方便地在本地部署大模型的软件，提供了包括模型下载与管理、API 等功能，本节我们简单讲述如何安装与使用 Ollama</p>
<p><img src="https://s2.loli.net/2025/02/18/NSUydrO6vWqsjXH.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>注：不包含 Windows 或 FreeBSD 等其他操作系统的方案，因为笔者不用这些系统：（</p>
</blockquote>
<h2 id="方法一：使用-Docker-进行部署（推荐）"><a href="#方法一：使用-Docker-进行部署（推荐）" class="headerlink" title="方法一：使用 Docker 进行部署（推荐）"></a>方法一：使用 Docker 进行部署（推荐）</h2><p>众所周知前面忘了中间忘了后面也忘了，总而言之使用 Docker 等带隔离机制的工具进行部署通常是更加安全的解决方案</p>
<p>首先将 ollama 官方的 docker 镜像拉到本地：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">docker pull ollama/ollama</span><br></code></pre></td></tr></table></figure>

<p>然后就能直接使用了？答案是肯定的也是否定的，虽然可以直接用 CPU 来运行本地模型，但是很明显用 GPU 跑是更快的，所以我们接下来还需要将显卡接入到容器当中</p>
<blockquote>
<p>如果你手上刚好没有显卡，或者你想只用 CPU 来跑本地模型，你也可以直接创建一个容器来运行：）</p>
</blockquote>
<h3 id="为-NVIDIA-GPU-进行配置"><a href="#为-NVIDIA-GPU-进行配置" class="headerlink" title="为 NVIDIA GPU 进行配置"></a>为 NVIDIA GPU 进行配置</h3><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/index.html">NVIDIA Container Toolkit</a> 是 NVIDIA 开发的能够让用户构建能使用 GPU 的容器的工具包，为了在容器当中使用 GPU，我们首先需要安装这一工具包</p>
<p><img src="https://s2.loli.net/2023/08/31/dWpAu8LJQ2bOShg.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>在 <a target="_blank" rel="noopener" href="https://arttnba3.cn/2023/08/31/OPS-0X00-DOCKER_ON_SERVER/">这篇博客</a> 当中笔者已经讲述了如何安装并使用 NVIDIA Container Toolkit 将显卡挂载到容器当中，这里我们就简单叙述一下如何在 SUSE 系的 Linux 发行版上完成这件事，首先我们需要添加 NVIDIA 的仓库：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">sudo zypper ar https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo</span><br></code></pre></td></tr></table></figure>

<p>接下来安装 NVIDIA Container Toolkit：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">sudo zypper --gpg-auto-import-keys install -y nvidia-container-toolkit</span><br></code></pre></td></tr></table></figure>

<blockquote>
<p>如果你使用的是 Gentoo 系统，可以直接安装社区移植到 Gentoo 仓库里的包：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">sudo emerge -av app-containers/nvidia-container-toolkit</span><br></code></pre></td></tr></table></figure>
</blockquote>
<p>然后配置 container runtime：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">sudo nvidia-ctk runtime configure --runtime=docker</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">sudo systemctl restart docker</span><br></code></pre></td></tr></table></figure>

<p>最后启动一个带 GPU 的 Ollama 容器，为了方便这里直接与本地共用网络：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">docker run -d --network=host --gpus=all -v ollama:/root/.ollama --name ollama ollama/ollama</span><br></code></pre></td></tr></table></figure>

<blockquote>
<p>如果你的机器上有多张 GPU，而你想让你的容器只使用其中某一张，可以将 <code>--gpus=all</code> 改为 <code>--gpus=&#39;&quot;device=显卡标号&quot;&#39;</code> 进行指定，其中显卡标号为使用 <code>nvidia-smi</code> 指令所获取的号码</p>
</blockquote>
<p>Ollama daemon 默认会使用 <code>11434</code> 端口提供 API，因此我们可以将这个端口映射出来使用：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama</span><br></code></pre></td></tr></table></figure>

<p>简单拉一个模型测试看看效果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">docker <span class="hljs-built_in">exec</span> -it ollama ollama pull llama3.2</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">curl http://localhost:11434/api/chat -d <span class="hljs-string">&#x27;&#123;</span></span><br>  &quot;model&quot;: &quot;llama3.2&quot;,<br>  &quot;messages&quot;: [<br>    &#123; &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好我是丁真&quot; &#125;<br>  ]<br>&#125;&#x27;<br></code></pre></td></tr></table></figure>

<p>成功运行，不过返回结果的格式稍微有些抽象：</p>
<p><img src="https://s2.loli.net/2025/02/18/V9lwj2XGtuOBNkY.png" srcset="/img/loading.gif" lazyload></p>
<p>当然，我们也可以通过命令行与 Ollama 进行交互，只需要运行如下命令进入交互界面：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">docker <span class="hljs-built_in">exec</span> -it ollama ollama run llama3.2</span><br></code></pre></td></tr></table></figure>

<p>然后就是一个交互式的聊天界面了，可以输入 <code>/bye</code> 退出：</p>
<p><img src="https://s2.loli.net/2025/03/12/dN5hUr2AKYn8pt7.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="为-AMD-GPU-进行配置（🕊）"><a href="#为-AMD-GPU-进行配置（🕊）" class="headerlink" title="为 AMD GPU 进行配置（🕊）"></a>为 AMD GPU 进行配置（🕊）</h3><p>笔者手上没有 AMD 的 GPU（短期内大概率应该也不会也没钱买），所以这一节先空着🕊🕊🕊🕊🕊🕊</p>
<blockquote>
<p>按照 <a target="_blank" rel="noopener" href="http://hub.docker.com/r/ollama/ollama">Ollama Docker</a> 的文档，可以运行如下命令创建带 AMD GPU 的容器，不过笔者没有试过，所以不保真：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">docker run -d --device /dev/kfd --device /dev/dri -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:rocm</span><br></code></pre></td></tr></table></figure>
</blockquote>
<h2 id="方法二：直接将-Ollama-安装到本地"><a href="#方法二：直接将-Ollama-安装到本地" class="headerlink" title="方法二：直接将 Ollama 安装到本地"></a>方法二：直接将 Ollama 安装到本地</h2><p>笔者比较推荐的是通过发行版对应的包管理器进行安装，例如笔者的服务器使用的是 openSUSE Tumbleweed，直接通过 zypper 安装即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">sudo zypper <span class="hljs-keyword">in</span> ollama</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">sudo systemctl <span class="hljs-built_in">enable</span> --now ollama</span><br></code></pre></td></tr></table></figure>

<p>我们也可以通过官网的安装脚本将 Ollama 直接安装到本地，只需要运行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">curl -fsSL https://ollama.com/install.sh | sh</span><br></code></pre></td></tr></table></figure>

<blockquote>
<p>需要注意的是这个方式安装的 Ollama 会直接安装到 <code>/usr/local/bin/lib/ollama</code> 或是其他类似路径，然后会直接删除旧 ollama 的二进制文件，非常不优雅也不受包管理器管控，笔者不太喜欢：（</p>
</blockquote>
<p>之后就能直接使用 ollama 了，这里还是简单随便测试一个模型：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">ollama pull llama3.2</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">curl http://localhost:11434/api/chat -d <span class="hljs-string">&#x27;&#123;</span></span><br>  &quot;model&quot;: &quot;llama3.2&quot;,<br>  &quot;messages&quot;: [<br>    &#123; &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好我不是丁真&quot; &#125;<br>  ]<br>&#125;&#x27;<br></code></pre></td></tr></table></figure>

<p>LLAMA3.2 的回答幽默程度超出笔者想象：</p>
<p><img src="https://s2.loli.net/2025/02/18/8CiYecmusFAN3aO.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="Example-部署-DeepSeek"><a href="#Example-部署-DeepSeek" class="headerlink" title="Example. 部署 DeepSeek"></a>Example. 部署 DeepSeek</h2><p>感觉其实没啥好讲的，直接用 Ollama 拉到本地就行，这里注意根据自己需求决定拉的版本的参数数量，笔者本地的显卡只有 6G 所以先拉一个 7B 版本的 DeepSeek-R1 试试水：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">ollama pull deepseek-r1:7b</span><br></code></pre></td></tr></table></figure>

<p>还是简单测试一下，感觉回答没有刚刚 LLAMA3.2 灵光一闪的那么幽默：</p>
<p><img src="https://s2.loli.net/2025/02/18/qvgJwxFi1N9AkZa.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="0x02-通过-Web-API-与-Ollama-进行交互"><a href="#0x02-通过-Web-API-与-Ollama-进行交互" class="headerlink" title="0x02. 通过 Web API 与 Ollama 进行交互"></a>0x02. 通过 Web API 与 Ollama 进行交互</h1><p>除了直接通过 shell 与 Ollama cli 聊天以外，我们还可以通过 Web API 与 Ollama 进行交互，Ollama 的基本架构如下图所示，其提供了一个 HTTP Server 供我们进行 HTTP 请求：</p>
<p><img src="https://s2.loli.net/2025/03/12/zHIKOY2BFACjDWy.png" srcset="/img/loading.gif" lazyload></p>
<p>对于部署在本地的 Ollama，其 Web 端口通常开放于 <code>11434</code> ，我们通常应当提交 POST 请求，下面我们看一些常见的 API 与例子</p>
<h2 id="通过-api-generate-进行简易交互"><a href="#通过-api-generate-进行简易交互" class="headerlink" title="通过 &#x2F;api&#x2F;generate 进行简易交互"></a>通过 &#x2F;api&#x2F;generate 进行简易交互</h2><p><code>/api/generate</code> 是一个简易的交互接口，用户可以通过该接口提供提示词让其生成单条回复，我们应当传入如下格式 JSON 数据作为输入：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;model&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;模型名&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;prompt&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;你的初始提示词&quot;</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure>

<p>以下是一个简易的例子：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">curl http://localhost:11434/api/generate -d <span class="hljs-string">&#x27;&#123;</span></span><br>  &quot;model&quot;: &quot;deepseek-r1:7b&quot;,<br>  &quot;prompt&quot;:&quot;我是丁真，我阿妈每天早上起来给我充瑞克五代&quot;<br>&#125;&#x27;<br></code></pre></td></tr></table></figure>

<p>可以看到 Ollama 的回答都是割裂成好多条的：</p>
<p><img src="https://s2.loli.net/2025/07/02/nZgSXMLAbRmlOeu.png" srcset="/img/loading.gif" lazyload></p>
<p>其回复通常遵循如下格式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 未结束，会回复多个如下 json</span><br>&#123;<br>    <span class="hljs-string">&quot;model&quot;</span>:<span class="hljs-string">&quot;模型名&quot;</span>,<br>    <span class="hljs-string">&quot;created_at&quot;</span>:<span class="hljs-string">&quot;生成时间&quot;</span>,<br>    <span class="hljs-string">&quot;response&quot;</span>:<span class="hljs-string">&quot;单个 token&quot;</span>,<br>    <span class="hljs-string">&quot;done&quot;</span>:false <span class="hljs-comment"># 是否终止</span><br>&#125;<br><br><span class="hljs-comment"># 已结束，会回复单个如下 json</span><br>&#123;<br>    <span class="hljs-string">&quot;model&quot;</span>:<span class="hljs-string">&quot;模型名&quot;</span>,<br>    <span class="hljs-string">&quot;created_at&quot;</span>:<span class="hljs-string">&quot;生成时间&quot;</span>,<br>    <span class="hljs-string">&quot;response&quot;</span>:<span class="hljs-string">&quot;&quot;</span>, <span class="hljs-comment"># 一定为空字符串</span><br>    <span class="hljs-string">&quot;done_reason&quot;</span>:<span class="hljs-string">&quot;stop&quot;</span>, <span class="hljs-comment"># 终止原因</span><br>    <span class="hljs-string">&quot;done&quot;</span>:true, <span class="hljs-comment"># 是否终止</span><br>    <span class="hljs-comment"># 剩下这些参数👴就不写注释了</span><br>    <span class="hljs-string">&quot;context&quot;</span>:[ <br>        <span class="hljs-comment"># 有很多数字的数组</span><br>    ],<br>    <span class="hljs-string">&quot;total_duration&quot;</span>: 数字,<br>    <span class="hljs-string">&quot;load_duration&quot;</span>:数字,<br>    <span class="hljs-string">&quot;prompt_eval_count&quot;</span>:数字,<br>    <span class="hljs-string">&quot;prompt_eval_duration&quot;</span>:数字,<br>    <span class="hljs-string">&quot;eval_count&quot;</span>:数字,<br>    <span class="hljs-string">&quot;eval_duration&quot;</span>:数字<br>&#125;<br><br></code></pre></td></tr></table></figure>



<h2 id="通过-api-chat-使用-ChatML-格式进行交互"><a href="#通过-api-chat-使用-ChatML-格式进行交互" class="headerlink" title="通过 &#x2F;api&#x2F;chat 使用 ChatML 格式进行交互"></a>通过 &#x2F;api&#x2F;chat 使用 ChatML 格式进行交互</h2><h3 id="基本对话"><a href="#基本对话" class="headerlink" title="基本对话"></a>基本对话</h3><p><code>/api/chat</code> 是最常用的交互接口，通常我们应当传递入以下格式的 JSON 数据：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;model&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;模型名&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;messages&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>        <span class="hljs-comment">/* 符合 OpenAI 格式的消息上下文 */</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure>

<p>其中 <code>&quot;messages&quot;</code> 参数我们需要传入符合 OpenAI 格式的消息上下文，即 <a target="_blank" rel="noopener" href="https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md">ChatML</a> （Chat Message Language） ，该格式虽然由 OpenAI 制定，但已经成为目前事实上的对话大模型标准，其基本结构应当为如下格式的数组：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">[<br>    &#123;<br>        <span class="hljs-string">&quot;role&quot;</span> : <span class="hljs-string">&quot;角色&quot;</span>,<br>        <span class="hljs-string">&quot;content&quot;</span> : <span class="hljs-string">&quot;消息&quot;</span><br>    &#125;<br>]<br></code></pre></td></tr></table></figure>

<p>各字段说明如下：</p>
<ul>
<li><code>&quot;role&quot;</code> ：该字段用以表示对话的角色，可选项有：<ul>
<li><code>&quot;system&quot;</code> ：系统消息，用以设定对话的初始背景</li>
<li><code>&quot;user&quot;</code> ：用户输入的消息，表示用户的提问</li>
<li><code>&quot;assistant&quot;</code> ：助手回答的消息，表示模型生成的回复</li>
</ul>
</li>
<li><code>&quot;content&quot;</code> ：该字段用以表示角色所说的内容</li>
</ul>
<p>例如以下是一个合法的请求：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">&#123;<br>    <span class="hljs-string">&quot;model&quot;</span>: <span class="hljs-string">&quot;deepseek-r1:7b&quot;</span>,<br>    <span class="hljs-string">&quot;messages&quot;</span> : [<br>        &#123;<br>            <span class="hljs-string">&quot;role&quot;</span> : <span class="hljs-string">&quot;system&quot;</span>,<br>            <span class="hljs-string">&quot;content&quot;</span> : <span class="hljs-string">&quot;你是游戏主播电棍，你现在正在进行直播，并针对观众的弹幕进行回复，你的回答应当符合他的直播风格&quot;</span><br>        &#125;,<br>        &#123;<br>            <span class="hljs-string">&quot;role&quot;</span> : <span class="hljs-string">&quot;user&quot;</span>,<br>            <span class="hljs-string">&quot;content&quot;</span> : <span class="hljs-string">&quot;你是职业选手吗&quot;</span><br>        &#125;<br>    ]<br>&#125;<br></code></pre></td></tr></table></figure>

<p>而 ollama 的返回格式通常遵循如下格式，每次回复一个 json 直到终止：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 未结束，会回复多个如下 json</span><br>&#123;<br>    <span class="hljs-string">&quot;model&quot;</span>:<span class="hljs-string">&quot;模型名&quot;</span>,<br>    <span class="hljs-string">&quot;created_at&quot;</span>:<span class="hljs-string">&quot;生成时间&quot;</span>,<br>    <span class="hljs-string">&quot;message&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;role&quot;</span>:<span class="hljs-string">&quot;assistant&quot;</span>,<br>        <span class="hljs-string">&quot;content&quot;</span>:<span class="hljs-string">&quot;单个 token&quot;</span><br>    &#125;,<br>    <span class="hljs-string">&quot;done&quot;</span>:false <span class="hljs-comment"># 是否终止</span><br>&#125;<br><br><span class="hljs-comment"># 已结束，会回复单个如下 json</span><br>&#123;<br>    <span class="hljs-string">&quot;model&quot;</span>:<span class="hljs-string">&quot;模型名&quot;</span>,<br>    <span class="hljs-string">&quot;created_at&quot;</span>:<span class="hljs-string">&quot;生成时间&quot;</span>,<br>    <span class="hljs-string">&quot;message&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;role&quot;</span>:<span class="hljs-string">&quot;assistant&quot;</span>,<br>        <span class="hljs-string">&quot;content&quot;</span>:<span class="hljs-string">&quot;&quot;</span> <span class="hljs-comment"># 一定为空字符串</span><br>    &#125;,<br>    <span class="hljs-string">&quot;done_reason&quot;</span>:<span class="hljs-string">&quot;stop&quot;</span>, <span class="hljs-comment"># 终止原因</span><br>    <span class="hljs-string">&quot;done&quot;</span>:true, <span class="hljs-comment"># 是否终止</span><br>    <span class="hljs-comment"># 剩下这些参数👴就不写注释了</span><br>    <span class="hljs-string">&quot;total_duration&quot;</span>: 数字,<br>    <span class="hljs-string">&quot;load_duration&quot;</span>:数字,<br>    <span class="hljs-string">&quot;prompt_eval_count&quot;</span>:数字,<br>    <span class="hljs-string">&quot;prompt_eval_duration&quot;</span>:数字,<br>    <span class="hljs-string">&quot;eval_count&quot;</span>:数字,<br>    <span class="hljs-string">&quot;eval_duration&quot;</span>:数字<br>&#125;<br><br></code></pre></td></tr></table></figure>

<p>使用 curl 编写请求还是太粗糙了，通常我们还是得将与模型间的交互整合到实际应用当中，下面是一个简易的 Python 示例程序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    req_data = &#123;<br>        <span class="hljs-string">&quot;model&quot;</span>: <span class="hljs-string">&quot;deepseek-r1:7b&quot;</span>,<br>        <span class="hljs-string">&quot;messages&quot;</span> : [<br>            &#123;<br>                <span class="hljs-string">&quot;role&quot;</span> : <span class="hljs-string">&quot;system&quot;</span>,<br>                <span class="hljs-string">&quot;content&quot;</span> : <span class="hljs-string">&quot;你是游戏主播电棍，你现在正在进行直播，并针对观众的弹幕进行回复，你的回答应当符合他的直播风格&quot;</span><br>            &#125;,<br>            &#123;<br>                <span class="hljs-string">&quot;role&quot;</span> : <span class="hljs-string">&quot;user&quot;</span>,<br>                <span class="hljs-string">&quot;content&quot;</span> : <span class="hljs-string">&quot;你是职业选手吗&quot;</span><br>            &#125;<br>        ]<br>    &#125;<br>    req_url = <span class="hljs-string">&#x27;http://localhost:11434/api/chat&#x27;</span><br><br>    reply = requests.post(req_url, data = json.dumps(req_data)).text.split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>    reply_msg = <span class="hljs-string">&#x27;&#x27;</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> reply:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(i) == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">continue</span><br>        msg = json.loads(i)<br>        reply_msg += msg[<span class="hljs-string">&#x27;message&#x27;</span>][<span class="hljs-string">&#x27;content&#x27;</span>]<br><br>    <span class="hljs-built_in">print</span>(reply_msg)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br><br></code></pre></td></tr></table></figure>

<p>因为模型的回答断断续续的，所以我们在 Python 脚本里拼了一下，可以看到 Deepseek 模型在回复中还会带有思考过程：</p>
<p><img src="https://s2.loli.net/2025/07/02/8COldZnPxqy6krj.png" srcset="/img/loading.gif" lazyload alt="电棍惨遭开除出职业选手籍"></p>
<h2 id="Example-将-Ollama-接入-QQ-bot"><a href="#Example-将-Ollama-接入-QQ-bot" class="headerlink" title="Example. 将 Ollama 接入 QQ bot"></a>Example. 将 Ollama 接入 QQ bot</h2><p>这里我们使用基于 <a target="_blank" rel="noopener" href="https://github.com/arttnba3/Shigure-Bot">Shigure-Bot</a> 这一 Bot SDK 开发的 Bot 应用 <a target="_blank" rel="noopener" href="https://github.com/arttnba3/Shione">Shione</a> 构建一个用于聊天的插件，Bot 后端使用兼容 <a target="_blank" rel="noopener" href="https://github.com/botuniverse/onebot-11/">OneBot 11 API</a> 的开源 NT QQ Protocol 的实现 <a target="_blank" rel="noopener" href="https://github.com/LagrangeDev/Lagrange.Core">Lagrange.Core</a> ，如何与这些框架进行对接以及具体使用方式留给读者课后自行阅读文档了（笑</p>
<p>首先编写一个核心函数用以处理模型的回复消息，这里笔者额外添加了一个在使用 Deepseek 时把思考过程删去的过程：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">ParseOLLAMAReply</span><span class="hljs-params">(rawReplyMsg []<span class="hljs-type">byte</span>)</span></span> (<span class="hljs-type">string</span>, <span class="hljs-type">error</span>) &#123;<br>	<span class="hljs-keyword">var</span> respMsg <span class="hljs-type">string</span><br><br>	<span class="hljs-comment">// original OLLAMA produce a list</span><br>	respJsonDataList := bytes.Split(rawReplyMsg, []<span class="hljs-type">byte</span>(<span class="hljs-string">&quot;\n&quot;</span>))<br>	<span class="hljs-keyword">for</span> _, respJsonData := <span class="hljs-keyword">range</span> respJsonDataList &#123;<br>		respJson := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-keyword">interface</span>&#123;&#125;)<br>		err := json.Unmarshal(respJsonData, &amp;respJson)<br>		<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>			<span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, errors.New(fmt.Sprintf(<span class="hljs-string">&quot;Error parsing reply: %v, original reply data:\n%v&quot;</span>, err.Error(), respJsonData))<br>		&#125;<br><br>		<span class="hljs-keyword">if</span> respJson[<span class="hljs-string">&quot;done&quot;</span>].(<span class="hljs-type">bool</span>) &#123;<br>			<span class="hljs-keyword">break</span><br>		&#125;<br><br>		respMsg += respJson[<span class="hljs-string">&quot;message&quot;</span>].(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-keyword">interface</span>&#123;&#125;)[<span class="hljs-string">&quot;content&quot;</span>].(<span class="hljs-type">string</span>)<br>	&#125;<br><br>	<span class="hljs-comment">// DeepSeek model will always output this, but we do not need...</span><br>	<span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(respMsg) &gt; <span class="hljs-built_in">len</span>(<span class="hljs-string">&quot;&lt;think&gt;&quot;</span>) &amp;&amp; respMsg[:<span class="hljs-built_in">len</span>(<span class="hljs-string">&quot;&lt;think&gt;&quot;</span>)] == <span class="hljs-string">&quot;&lt;think&gt;&quot;</span> &#123;<br>		splitRes := strings.Split(respMsg, <span class="hljs-string">&quot;&lt;/think&gt;\n&quot;</span>)<br>		<span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(splitRes) &gt; <span class="hljs-number">1</span> &#123;<br>			respMsg = splitRes[<span class="hljs-number">1</span>]<br>		&#125;<br>	&#125;<br><br>	<span class="hljs-keyword">return</span> respMsg, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure>

<p>然后是消息请求函数，我们将请求直接发给对应路径即可：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">RequestModel</span><span class="hljs-params">(provider <span class="hljs-type">string</span>, url <span class="hljs-type">string</span>, model <span class="hljs-type">string</span>, headers <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-keyword">interface</span>&#123;&#125;, maxWaitingTime time.Duration, messages <span class="hljs-keyword">interface</span>&#123;&#125;)</span></span> ([]<span class="hljs-type">byte</span>, <span class="hljs-type">error</span>) &#123;<br>    reqData := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-keyword">interface</span>&#123;&#125;)<br>	reqData[<span class="hljs-string">&quot;model&quot;</span>] = model<br>	reqData[<span class="hljs-string">&quot;messages&quot;</span>] = messages<br><br>	reqBodyJson, err := json.Marshal(reqData)<br>	<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>		<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>	&#125;<br><br>	req, err := http.NewRequest(<span class="hljs-string">&quot;POST&quot;</span>, url, bytes.NewBuffer(reqBodyJson))<br>	<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>		<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>	&#125;<br><br>	req.Header.Set(<span class="hljs-string">&quot;Content-Type&quot;</span>, <span class="hljs-string">&quot;application/json&quot;</span>)<br>	<span class="hljs-keyword">if</span> headers != <span class="hljs-literal">nil</span> &#123;<br>		<span class="hljs-keyword">for</span> k, v := <span class="hljs-keyword">range</span> headers &#123;<br>			req.Header.Set(k, v.(<span class="hljs-type">string</span>))<br>		&#125;<br>	&#125;<br><br>	client := &amp;http.Client&#123;<br>		Timeout: maxWaitingTime,<br>	&#125;<br>	resp, err := client.Do(req)<br>	<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>		<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>	&#125;<br><br>	<span class="hljs-keyword">defer</span> resp.Body.Close()<br>    <br>    <span class="hljs-keyword">if</span> resp.StatusCode != <span class="hljs-number">200</span> &#123;<br>		<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, errors.New(<span class="hljs-string">&quot;Response status code is &quot;</span> + strconv.Itoa(resp.StatusCode))<br>	&#125;<br><br>	respBody, err := io.ReadAll(resp.Body)<br>	<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>		<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>	&#125;<br>    <br>    <span class="hljs-keyword">return</span> respBody, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure>

<p>之后在上面再套一层，其根据不同的供应商选择调用不同的消息处理函数，这里我们只有 Ollama 所以只需要调用前面写的 <code>ParseOllamaReply</code>：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">ChatWithAI</span><span class="hljs-params">(provider <span class="hljs-type">string</span>, url <span class="hljs-type">string</span>, model <span class="hljs-type">string</span>, prompt <span class="hljs-type">string</span>, headers <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-keyword">interface</span>&#123;&#125;, maxWaitingTime time.Duration, messages <span class="hljs-keyword">interface</span>&#123;&#125;)</span></span> (<span class="hljs-type">string</span>, <span class="hljs-type">error</span>) &#123;<br>	<span class="hljs-keyword">var</span> respMsg <span class="hljs-type">string</span><br>    <br>    respBody, err := RequestModel(provider, url, model, headers, maxWaitingTime, messages)<br>    <span class="hljs-keyword">if</span> err != nill &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, err<br>    &#125;<br><br>	<span class="hljs-keyword">switch</span> provider &#123;<br>	<span class="hljs-keyword">case</span> <span class="hljs-string">&quot;Ollama&quot;</span>:<br>		respMsg, err = ParseOLLAMAReply(respBody)<br>		<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>			<span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, err<br>		&#125;<br>		<span class="hljs-keyword">break</span><br>	<span class="hljs-keyword">default</span>:<br>		<span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, errors.New(<span class="hljs-string">&quot;Unknown provider: &quot;</span> + provider)<br>	&#125;<br><br>	<span class="hljs-keyword">return</span> respMsg, <span class="hljs-literal">nil</span><br>&#125;<br><br></code></pre></td></tr></table></figure>

<p>之后可以自己预先构筑一个想要的上下文作为输入来构造一个自己想要的 bot 人格，下面简单测试一下：</p>
<p><img src="https://s2.loli.net/2025/07/03/8GlJm7Obg5wrMdt.png" srcset="/img/loading.gif" lazyload></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/OPS/" class="category-chain-item">OPS</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Linux/">#Linux</a>
      
        <a href="/tags/%E8%BF%90%E7%BB%B4/">#运维</a>
      
        <a href="/tags/DeepSeek/">#DeepSeek</a>
      
        <a href="/tags/Ollama/">#Ollama</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>【OPS.0x06】使用 Ollama 本地部署轻量LLM</div>
      <div>https://arttnba3.github.io/2025/05/31/OPS-0X06-DEPLOY_OLLAMA/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>arttnba3</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年5月31日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/06/04/CTF-0X0A_D3CTF2025_D3KHEAP2_D3KSHRM/" title="【CTF.0x0A】D^ 3CTF2025 d3kheap2、d3kshrm 出题手记">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">【CTF.0x0A】D^ 3CTF2025 d3kheap2、d3kshrm 出题手记</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/04/30/CVE-0X0D-CVE-2021-22600/" title="【CVE.0x0D】CVE-2021-22600 漏洞分析及利用">
                        <span class="hidden-mobile">【CVE.0x0D】CVE-2021-22600 漏洞分析及利用</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.4.17/Valine.min.js', function() {
        var options = Object.assign(
          {"appid":"ICj6cPRQWFTNiOttBHlzxnIv-gzGzoHsz","appkey":"tuvJh3xYxPFcW2JB6K26RKP2","path":"window.location.pathname","placeholder":"说点什么呗（笑）","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"appId":"ICj6cPRQWFTNiOttBHlzxnIv-gzGzoHsz","appKey":"tuvJh3xYxPFcW2JB6K26RKP2"},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  <!-- 网站运行时间的设置 -->
  <span id="timeDate">载入天数...</span>
  <span id="times">载入时分秒...</span>
  <script>
      var now = new Date();
      function createtime() {
          var grt= new Date("04/20/2020 17:48:48");//此处修改你的建站时间或者网站上线时间
          now.setTime(now.getTime()+250);
          days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
          hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
          if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
          mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
          seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
          snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
          document.getElementById("timeDate").innerHTML = "arttnba3的小屋已经安全存在了 "+dnum+" 天 ";
          document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
      }
  setInterval("createtime()",250);
  </script>
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
    <!-- 备案信息 ICP for China -->
    <div class="beian">
  <span>
    <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
      桂ICP备2022005068号-1
    </a>
  </span>
  
</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
